{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6175b779-cbb8-4544-bf76-23ec4f74352f",
   "metadata": {},
   "source": [
    "# Project: Iris Dataset ðŸŒ¸\n",
    "\n",
    "- **Project Name:** Iris Classification Project\n",
    "- **Project Type:** Multi-class Classification\n",
    "- **Author:** Dr. Saad Laouadi\n",
    "\n",
    "### Project Overview:\n",
    "This project leverages the famous **Iris Dataset** for **multi-class classification**, focusing on identifying the species of iris flowers based on their petal and sepal measurements.\n",
    "\n",
    "### Key Features:\n",
    "- **Classification Task**: Predict the species of iris flowers (Setosa, Versicolor, Virginica)\n",
    "- **Algorithms Used**: [Specify any algorithms or models you've implemented]\n",
    "- **Evaluation Metrics**: [List metrics like accuracy, precision, F1-score, etc.]\n",
    "\n",
    "---\n",
    "\n",
    "**Copyright Â© Dr. Saad Laouadi**  \n",
    "**All Rights Reserved** ðŸ›¡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e16b78ea-01cf-4412-ad8c-6010792a4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingClassifier, \n",
    "    RandomForestClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    ExtraTreesClassifier, \n",
    "    BaggingClassifier\n",
    ")\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Data paths\n",
    "DATA_URL = \"https://raw.githubusercontent.com/dr-saad-la/ML-Datasets/refs/heads/main/benchmark-ml-datasets/iris.csv\"\n",
    "\n",
    "# Retrieve the base path from the environment variable\n",
    "BASE_LOCAL_PATH = Path(os.getenv('DATA_PATH'))\n",
    "\n",
    "# Ensure the environment variable is set, then create the full local path using pathlib\n",
    "if BASE_LOCAL_PATH:\n",
    "    LOCAL_PATH = BASE_LOCAL_PATH.joinpath(\"ML-Datasets/benchmark-ml-datasets/iris.csv\")\n",
    "else:\n",
    "    print(\"no environment variable is found\")    \n",
    "\n",
    "# Suppress ConvergenceWarning\n",
    "warnings.filterwarnings('ignore', category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb2a89e5-b81d-4013-91dc-2cf830346c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(data_source: str, data_url: str, local_path: Path, base_path: Path = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset from either a URL or a local file, based on the selected data source.\n",
    "\n",
    "    Parameters:\n",
    "    - data_source (str): Choose 'url' for online dataset or 'local' for local file.\n",
    "    - data_url (str): The URL to load the dataset from if 'url' is selected.\n",
    "    - local_path (Path): The local file path to load the dataset from if 'local' is selected.\n",
    "    - base_path (Path): The base directory to strip when printing the local path (optional).\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The loaded dataset as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    if data_source == 'url':\n",
    "        print(\"Loading data from URL...\")\n",
    "        return pd.read_csv(data_url)\n",
    "    elif local_path.exists():  \n",
    "        if base_path:\n",
    "            relative_path = local_path.relative_to(base_path)\n",
    "        else:\n",
    "            relative_path = local_path\n",
    "        print(f\"Loading data from local path: /{relative_path}\")\n",
    "        return pd.read_csv(local_path)\n",
    "    else:\n",
    "        print(f\"Error: The local file was not found at /{relative_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_data(df):\n",
    "    # Step 1: Encode the 'class' column (categorical) into numerical labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    df['class'] = label_encoder.fit_transform(df['class'])\n",
    "    \n",
    "    # Step 2: Separate features (X) and target (y)\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "    \n",
    "    # Step 3: Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    \n",
    "    # Step 4: Normalize the feature data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "\n",
    "def train_knn(X_train, X_test, y_train, y_test, n_neighbors=5):\n",
    "    # Step 1: Initialize the KNN model\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = knn.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"KNN Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    # Step 1: Initialize the Logistic Regression model\n",
    "    logistic_regression = LogisticRegression(max_iter=1000)  \n",
    "    # Step 2: Train the model on the training data\n",
    "    logistic_regression.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = logistic_regression.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# Function for hyperparameter tuning of Logistic Regression\n",
    "def tune_logistic_regression(X_train, y_train):\n",
    "    # Step 1: Define the Logistic Regression model\n",
    "    logistic_regression = LogisticRegression(max_iter=1000)\n",
    "    \n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear', 'saga'],  # Different solvers for optimization\n",
    "        'C': [0.01, 0.1, 1.0, 10.0, 100.0],                     # Regularization strength (inverse)\n",
    "        'penalty': ['l2']                                       # L2 regularization (for solvers supporting it)\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=logistic_regression,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,       # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate the SVM model\n",
    "def train_svm(X_train, X_test, y_train, y_test, kernel='linear', C=1.0):\n",
    "    # Step 1: Initialize the SVM model\n",
    "    svm_model = SVC(kernel=kernel, C=C)  # 'kernel' specifies the type of SVM, 'C' is the regularization parameter\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = svm_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"SVM Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Function for hyperparameter tuning of SVM\n",
    "def tune_svm(X_train, y_train):\n",
    "    # Step 1: Define the SVC model\n",
    "    svm_model = SVC()\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],  # Different kernels for the SVM\n",
    "        'C': [0.1, 1.0, 10, 100],  # Regularization parameter\n",
    "        'gamma': ['scale', 'auto'],  # Kernel coefficient for 'rbf', 'poly', 'sigmoid'\n",
    "        'degree': [2, 3, 4]  # Only for 'poly' kernel\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=svm_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Print progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate GradientBoostingClassifier\n",
    "def train_gradient_boosting(X_train, X_test, y_train, y_test, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "    \"\"\"\n",
    "    Train and evaluate a GradientBoostingClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - n_estimators: The number of boosting stages to be run\n",
    "    - learning_rate: Shrinks the contribution of each tree by this factor\n",
    "    - max_depth: Maximum depth of each tree\n",
    "\n",
    "    Returns:\n",
    "    - Trained GradientBoostingClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the Gradient Boosting model\n",
    "    gb_model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = gb_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Gradient Boosting Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return gb_model\n",
    "\n",
    "# Function for hyperparameter tuning of GradientBoostingClassifier\n",
    "def tune_gradient_boosting(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for GradientBoostingClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the Gradient Boosting model\n",
    "    gb_model = GradientBoostingClassifier()\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  # Number of boosting stages\n",
    "        'learning_rate': [0.01, 0.1, 0.2],  # Learning rate for shrinking the contribution of each tree\n",
    "        'max_depth': [3, 4, 5],  # Maximum depth of each tree\n",
    "        'subsample': [0.8, 1.0],  # Fraction of samples used for fitting individual base learners\n",
    "        'min_samples_split': [2, 5, 10]  # Minimum number of samples required to split an internal node\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=gb_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate RandomForestClassifier\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, n_estimators=100, max_depth=None, min_samples_split=2):\n",
    "    \"\"\"\n",
    "    Train and evaluate a RandomForestClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - n_estimators: The number of trees in the forest\n",
    "    - max_depth: Maximum depth of the trees (None for no restriction)\n",
    "    - min_samples_split: Minimum number of samples required to split a node\n",
    "\n",
    "    Returns:\n",
    "    - Trained RandomForestClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the Random Forest model\n",
    "    rf_model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return rf_model\n",
    "\n",
    "# Function for hyperparameter tuning of RandomForestClassifier\n",
    "def tune_random_forest(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for RandomForestClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the Random Forest model\n",
    "    rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "        'bootstrap': [True, False]  # Whether to use bootstrapping\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate AdaBoostClassifier\n",
    "def train_adaboost(X_train, X_test, y_train, y_test, n_estimators=50, learning_rate=1.0):\n",
    "    \"\"\"\n",
    "    Train and evaluate an AdaBoostClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - n_estimators: The number of weak learners (default is 50)\n",
    "    - learning_rate: Shrinks the contribution of each classifier by this factor (default is 1.0)\n",
    "\n",
    "    Returns:\n",
    "    - Trained AdaBoostClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the AdaBoost model\n",
    "    adaboost_model = AdaBoostClassifier(n_estimators=n_estimators, \n",
    "                                        learning_rate=learning_rate, \n",
    "                                        random_state=0,\n",
    "                                       algorithm=\"SAMME\")\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    adaboost_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = adaboost_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"AdaBoost Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return adaboost_model\n",
    "\n",
    "# Function for hyperparameter tuning of AdaBoostClassifier\n",
    "def tune_adaboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for AdaBoostClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the AdaBoost model\n",
    "    adaboost_model = AdaBoostClassifier(random_state=0, algorithm = \"SAMME\")\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300, 500, 750, 1000, 1500],  # Number of weak learners (trees)\n",
    "        'learning_rate': [0.01, 0.1, 0.5, 1.0]  # Shrinkage of weak learners' contribution\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=adaboost_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate ExtraTreesClassifier\n",
    "def train_extra_trees(X_train, X_test, y_train, y_test, n_estimators=100, max_depth=None, min_samples_split=2):\n",
    "    \"\"\"\n",
    "    Train and evaluate an ExtraTreesClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - n_estimators: The number of trees in the forest (default is 100)\n",
    "    - max_depth: Maximum depth of the trees (None for no restriction)\n",
    "    - min_samples_split: Minimum number of samples required to split a node (default is 2)\n",
    "\n",
    "    Returns:\n",
    "    - Trained ExtraTreesClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the Extra Trees model\n",
    "    et_model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    et_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = et_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Extra Trees Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return et_model\n",
    "\n",
    "# Function for hyperparameter tuning of ExtraTreesClassifier\n",
    "def tune_extra_trees(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for ExtraTreesClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the Extra Trees model\n",
    "    et_model = ExtraTreesClassifier(random_state=42)\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of each tree\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4],  # Minimum samples required at a leaf node\n",
    "        'bootstrap': [True, False]  # Whether to use bootstrapping\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=et_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate MLPClassifier\n",
    "def train_mlp(X_train, X_test, y_train, y_test, hidden_layer_sizes=(100,), max_iter=200, learning_rate_init=0.001):\n",
    "    \"\"\"\n",
    "    Train and evaluate an MLPClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - hidden_layer_sizes: The number of neurons in the hidden layers (default is (100,))\n",
    "    - max_iter: Maximum number of iterations for training (default is 200)\n",
    "    - learning_rate_init: The initial learning rate for weight updates (default is 0.001)\n",
    "\n",
    "    Returns:\n",
    "    - Trained MLPClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the MLP model\n",
    "    mlp_model = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes, max_iter=max_iter, learning_rate_init=learning_rate_init, random_state=42)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    mlp_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = mlp_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"MLP Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return mlp_model\n",
    "\n",
    "# Function for hyperparameter tuning of MLPClassifier\n",
    "def tune_mlp(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for MLPClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the MLP model\n",
    "    mlp_model = MLPClassifier(random_state=42)\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(100,), (100, 50), (50, 50)],  # Number of neurons in hidden layers\n",
    "        'max_iter': [1000, 1500],                             # Maximum number of iterations\n",
    "        'learning_rate_init': [0.001, 0.01, 0.1]              # Initial learning rate\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=mlp_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Function to train and evaluate BaggingClassifier\n",
    "def train_bagging(X_train, X_test, y_train, y_test, base_estimator=None, n_estimators=10):\n",
    "    \"\"\"\n",
    "    Train and evaluate a BaggingClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - base_estimator: The base model to use for bagging (default is DecisionTreeClassifier)\n",
    "    - n_estimators: The number of base estimators (default is 10)\n",
    "    \n",
    "    Returns:\n",
    "    - Trained BaggingClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the base estimator (default is DecisionTreeClassifier)\n",
    "    if base_estimator is None:\n",
    "        base_estimator = DecisionTreeClassifier()\n",
    "    \n",
    "    # Step 2: Initialize the Bagging model\n",
    "    bagging_model = BaggingClassifier(estimator=base_estimator, n_estimators=n_estimators, random_state=42)\n",
    "    \n",
    "    # Step 3: Train the model on the training data\n",
    "    bagging_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 4: Make predictions on the test data\n",
    "    y_pred = bagging_model.predict(X_test)\n",
    "    \n",
    "    # Step 5: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 6: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return bagging_model\n",
    "\n",
    "# Function for hyperparameter tuning of BaggingClassifier\n",
    "def tune_bagging(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for BaggingClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the Bagging model\n",
    "    base_estimator = DecisionTreeClassifier()\n",
    "    bagging_model = BaggingClassifier(estimator=base_estimator, random_state=42)\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [10, 50, 100],  # Number of base estimators\n",
    "        'max_samples': [0.5, 0.7, 1.0],  # Proportion of samples used for training\n",
    "        'max_features': [0.5, 0.7, 1.0],  # Proportion of features used for training\n",
    "        'bootstrap': [True, False],  # Whether to use bootstrapping for sampling\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=bagging_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85351c9f-f2c1-4dd7-af41-4a55ac1ccd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from local path: /ML-Datasets/benchmark-ml-datasets/iris.csv\n",
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "# Choose data source: 'url' for online dataset, 'local' for local file\n",
    "data_source = 'local'  # CHANGE TO 'url' IF YOU YOU LOAD FROM THE `URL`\n",
    "\n",
    "# Load the datasets\n",
    "data = load_dataset(data_source, DATA_URL, LOCAL_PATH, BASE_LOCAL_PATH)\n",
    "\n",
    "# Display the first few rows of the dataset (if loaded successfully)\n",
    "if data is not None:\n",
    "    print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5ab82a-64e6-46ac-ae37-16cea3cdf270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6af954ca-e8b0-4d15-adbe-dbd00c9f6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data\n",
    "X_train_scaled, X_test_scaled, y_train, y_test = process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59391e92-75a3-402e-8536-f773f14fb200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_scaled shape: (105, 4)\n",
      "y_train shape: (105,)\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows of the processed data\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a8cebb5-84de-493b-abc9-e61c16f4f230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the KNN model\n",
    "train_knn(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23046fbe-feee-4dc7-b0cb-3565df56b407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model\n",
    "train_logistic_regression(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4eba9f9-bd2d-4ef4-a372-17d8c5155e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best Hyperparameters: {'C': 100.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "Best Cross-Validation Accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Perform hyperparameter tuning\n",
    "best_logistic_regression = tune_logistic_regression(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25d96a81-6518-4a2c-a082-29a3b78b3337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Use the best model to make predictions and evaluate\n",
    "y_pred = best_logistic_regression.predict(X_test_scaled)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e88afdc-e8e2-4b0c-bfac-ae31180456db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the SVM model\n",
    "train_svm(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9376da2a-8709-4722-8715-e7123531fd2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best Hyperparameters: {'C': 100, 'degree': 2, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Cross-Validation Accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# find the best svm classifier hyperparameters\n",
    "best_svm = tune_svm(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2102a71-22f3-4824-b347-af8095aa0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_svm.predict(X_test_scaled)\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41611ba0-aef4-4d2e-a360-b19b2b894e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Gradient Boosting model\n",
    "gb_model = train_gradient_boosting(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b92bef9d-b15a-4d64-a5b7-6ff8a475f5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for GradientBoostingClassifier\n",
    "best_gb_model = tune_gradient_boosting(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce7a44e4-98ce-4b6e-b18d-f7b1051739b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_gb_model.predict(X_test_scaled)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a27b0304-06ea-4f25-8c95-9dcf92df072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Random Forest model\n",
    "rf_model = train_random_forest(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e57a57dc-0a3e-4edf-926a-fa0732f6881e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Accuracy: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for RandomForestClassifier\n",
    "best_rf_model = tune_random_forest(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5751a63c-a083-416e-ba27-289736e1aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Use the best model to make predictions and evaluate\n",
    "y_pred = best_rf_model.predict(X_test_scaled)\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0eb7eae7-7c8f-4eba-827e-17a24ef8f5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.89      0.94      0.92        18\n",
      "           2       0.90      0.82      0.86        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.92      0.93        45\n",
      "weighted avg       0.93      0.93      0.93        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the AdaBoost model\n",
    "adaboost_model = train_adaboost(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f0f6e36-2ce5-4c74-8f44-aa829cf6b6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "Best Hyperparameters: {'learning_rate': 0.01, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.9619\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for AdaBoostClassifier\n",
    "best_adaboost_model = tune_adaboost(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d181a35-8035-4259-aa11-cbe2343f8e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 0.9111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.85      0.94      0.89        18\n",
      "           2       0.89      0.73      0.80        11\n",
      "\n",
      "    accuracy                           0.91        45\n",
      "   macro avg       0.91      0.89      0.90        45\n",
      "weighted avg       0.91      0.91      0.91        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_adaboost_model.predict(X_test_scaled)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b43da1cf-0ffd-4e73-bc4d-8db39f9569a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Train and evaluate the Extra Trees model\n",
    "et_model = train_extra_trees(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40d9f8dc-c505-4cea-b68f-b1f908510e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "Best Hyperparameters: {'bootstrap': False, 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for ExtraTreesClassifier\n",
    "best_et_model = tune_extra_trees(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae20bcca-0a91-472c-90c7-34b2bc28a4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_et_model.predict(X_test_scaled)\n",
    "print(f\"Extra Trees Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f0b1a2a-f6ba-4e72-9874-c3ff2888c4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the MLP model\n",
    "mlp_model = train_mlp(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b296b08f-6cf4-4d2a-a1f1-32668dd4f5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'hidden_layer_sizes': (100,), 'learning_rate_init': 0.1, 'max_iter': 1000}\n",
      "Best Cross-Validation Accuracy: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for MLPClassifier\n",
    "best_mlp_model = tune_mlp(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fbcf737-6985-4973-86e2-e692452381c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_mlp_model.predict(X_test_scaled)\n",
    "print(f\"MLP Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "036b9844-65cc-495a-ab27-3c7fdfd6e9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Bagging model\n",
    "bagging_model = train_bagging(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "979bc6ef-8f42-40a0-acf0-107be3d4884f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n",
      "Best Hyperparameters: {'bootstrap': True, 'max_features': 0.5, 'max_samples': 0.7, 'n_estimators': 50}\n",
      "Best Cross-Validation Accuracy: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for BaggingClassifier\n",
    "best_bagging_model = tune_bagging(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d61773aa-9de5-47e5-8753-b688e4756869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_bagging_model.predict(X_test_scaled)\n",
    "print(f\"Bagging Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21ea2edf-f00a-4158-b2ac-58165abe6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traing CatBoost Classifier\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7612ecc1-0e5a-49d5-be6c-94a186cc59c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_catboost(X_train, X_test, y_train, y_test, iterations=1000, learning_rate=0.1, depth=6):\n",
    "    \"\"\"\n",
    "    Train and evaluate a CatBoostClassifier.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Scaled training and testing features\n",
    "    - y_train, y_test: Training and testing labels\n",
    "    - iterations: The number of boosting iterations (default is 1000)\n",
    "    - learning_rate: Learning rate for the model (default is 0.1)\n",
    "    - depth: Depth of the trees (default is 6)\n",
    "    \n",
    "    Returns:\n",
    "    - Trained CatBoostClassifier\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize the CatBoost model\n",
    "    catboost_model = CatBoostClassifier(iterations=iterations, \n",
    "                                        learning_rate=learning_rate, \n",
    "                                        depth=depth,\n",
    "                                        verbose=100, \n",
    "                                        random_state=0)\n",
    "    \n",
    "    # Step 2: Train the model on the training data\n",
    "    catboost_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 3: Make predictions on the test data\n",
    "    y_pred = catboost_model.predict(X_test)\n",
    "    \n",
    "    # Step 4: Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"CatBoost Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Step 5: Print a classification report\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return catboost_model\n",
    "\n",
    "# Function for hyperparameter tuning of CatBoostClassifier\n",
    "def tune_catboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for CatBoostClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Scaled training features\n",
    "    - y_train: Training labels\n",
    "    \n",
    "    Returns:\n",
    "    - Best estimator after tuning\n",
    "    \"\"\"\n",
    "    # Step 1: Define the CatBoost model\n",
    "    catboost_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "\n",
    "    # Step 2: Set the hyperparameter grid\n",
    "    param_grid = {\n",
    "        'iterations': [500, 1000],  # Number of boosting iterations\n",
    "        'learning_rate': [0.01, 0.05, 0.1],  # Learning rate\n",
    "        'depth': [4, 6, 8],  # Depth of the trees\n",
    "    }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=catboost_model,\n",
    "                               param_grid=param_grid,\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,  # Show progress\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Print the best parameters and best score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67bc02cd-4352-453e-b426-20dd0b3e1bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.9827943\ttotal: 56.8ms\tremaining: 56.7s\n",
      "100:\tlearn: 0.0472036\ttotal: 82.4ms\tremaining: 733ms\n",
      "200:\tlearn: 0.0197420\ttotal: 107ms\tremaining: 424ms\n",
      "300:\tlearn: 0.0122758\ttotal: 132ms\tremaining: 306ms\n",
      "400:\tlearn: 0.0089466\ttotal: 157ms\tremaining: 235ms\n",
      "500:\tlearn: 0.0070530\ttotal: 185ms\tremaining: 184ms\n",
      "600:\tlearn: 0.0057753\ttotal: 210ms\tremaining: 139ms\n",
      "700:\tlearn: 0.0048400\ttotal: 234ms\tremaining: 100ms\n",
      "800:\tlearn: 0.0041801\ttotal: 260ms\tremaining: 64.5ms\n",
      "900:\tlearn: 0.0036865\ttotal: 287ms\tremaining: 31.5ms\n",
      "999:\tlearn: 0.0032939\ttotal: 311ms\tremaining: 0us\n",
      "CatBoost Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the CatBoost model\n",
    "catboost_model = train_catboost(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "402daed8-682f-414e-8f1b-747235ce4f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best Hyperparameters: {'depth': 4, 'iterations': 500, 'learning_rate': 0.01}\n",
      "Best Cross-Validation Accuracy: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for CatBoostClassifier\n",
    "best_catboost_model = tune_catboost(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dcb24e0b-dca8-4054-a760-c6ff01afd79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_catboost_model.predict(X_test_scaled)\n",
    "print(f\"CatBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc06bb52-02cc-48eb-91d2-4dd2f66a348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost Classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgboost(X_train, X_test, y_train, y_test, learning_rate=0.1, n_estimators=100, max_depth=3, subsample=1.0, colsample_bytree=1.0):\n",
    "    \"\"\"\n",
    "    Train an XGBoostClassifier with specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Training and testing features (numpy arrays or DataFrames)\n",
    "    - y_train, y_test: Training and testing labels (numpy arrays or Series)\n",
    "    - learning_rate: Step size shrinkage used to prevent overfitting (default: 0.1)\n",
    "    - n_estimators: Number of boosting rounds (default: 100)\n",
    "    - max_depth: Maximum depth of the tree (default: 3)\n",
    "    - subsample: Subsample ratio of the training data (default: 1.0)\n",
    "    - colsample_bytree: Subsample ratio of columns when constructing trees (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained XGBoostClassifier model\n",
    "    \"\"\"\n",
    "    # Initialize the XGBoost model with the specified hyperparameters\n",
    "    xgb_model = XGBClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        eval_metric='mlogloss',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"XGBoost Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return xgb_model\n",
    "\n",
    "def tune_xgboost(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for XGBoostClassifier using GridSearchCV.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features (numpy arrays or DataFrames)\n",
    "    - y_train: Training labels (numpy arrays or Series)\n",
    "    \n",
    "    Returns:\n",
    "    - best_model: XGBoostClassifier with the best hyperparameters\n",
    "    \"\"\"\n",
    "    # Initialize the base model\n",
    "    xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    \n",
    "    # Define the hyperparameter grid to search\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=xgb_model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='accuracy',\n",
    "                               cv=5,  # 5-fold cross-validation\n",
    "                               verbose=1,\n",
    "                               n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Fit GridSearchCV on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters and score\n",
    "    print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Return the best model\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "def train_and_tune_xgboost(X_train, X_test, y_train, y_test, param_grid=None):\n",
    "    \"\"\"\n",
    "    Train and fine-tune an XGBoostClassifier using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, X_test: Features for training and testing (numpy arrays or DataFrames)\n",
    "    - y_train, y_test: Labels for training and testing (numpy arrays or Series)\n",
    "    - param_grid: Dictionary of hyperparameters to search for the best model (default is None, which uses basic hyperparameters)\n",
    "\n",
    "    Returns:\n",
    "    - best_model: The XGBoostClassifier with the best hyperparameters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Initialize the base XGBoostClassifier model\n",
    "    xgb_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "    \n",
    "    # Step 2: If no param_grid is provided, define a default one\n",
    "    if param_grid is None:\n",
    "        param_grid = {\n",
    "            'learning_rate': [0.01, 0.05, 0.1],   # Learning rate\n",
    "            'n_estimators': [100, 200, 500],      # Number of boosting rounds\n",
    "            'max_depth': [3, 5, 7],               # Maximum depth of a tree\n",
    "            'subsample': [0.6, 0.8, 1.0],         # Subsampling ratio of the training data\n",
    "            'colsample_bytree': [0.6, 0.8, 1.0]   # Subsampling ratio of columns when constructing trees\n",
    "        }\n",
    "    \n",
    "    # Step 3: Initialize GridSearchCV with 5-fold cross-validation\n",
    "    grid_search = GridSearchCV(estimator=xgb_model,\n",
    "                               param_grid=param_grid,\n",
    "                               scoring='accuracy',  # Evaluate using accuracy\n",
    "                               cv=5,                # 5-fold cross-validation\n",
    "                               verbose=1,           # Show progress\n",
    "                               n_jobs=-1)           # Use all available cores\n",
    "    \n",
    "    # Step 4: Fit GridSearchCV on the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Step 5: Retrieve the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Step 6: Make predictions on the test data using the best model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Step 7: Evaluate the model using accuracy score and classification report\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"XGBoost Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Step 8: Return the best model\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44c4a434-de7d-40a4-b624-8cbd7c7d3e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  Train and fine-tune XGBoost\n",
    "best_xgboost_model = train_xgboost(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62a91f36-d1fd-49a7-97f4-8870013cf35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "Best Hyperparameters: {'colsample_bytree': 0.6, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.6}\n",
      "Best Cross-Validation Accuracy: 0.9524\n"
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for XGBoostClassifier\n",
    "best_xgb_model = tune_xgboost(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1499b0e8-3172-4f6b-b70d-ea9459291fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_xgb_model.predict(X_test_scaled)\n",
    "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "757e9f8e-4315-41fb-b1ec-3fd135696b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "XGBoost Accuracy: 0.9778\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = train_and_tune_xgboost(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c1c366e-5b99-4861-984d-f2542d333eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LGBMClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "519213ec-2b04-4900-b435-21f65a5a3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(X_train, X_test, y_train, y_test, learning_rate=0.1, n_estimators=100, max_depth=-1, subsample=1.0, colsample_bytree=1.0):\n",
    "    \"\"\"\n",
    "    Train an LGBMClassifier with specified hyperparameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train, X_test: Training and testing features (numpy arrays or DataFrames)\n",
    "    - y_train, y_test: Training and testing labels (numpy arrays or Series)\n",
    "    - learning_rate: Step size shrinkage used to prevent overfitting (default: 0.1)\n",
    "    - n_estimators: Number of boosting rounds (default: 100)\n",
    "    - max_depth: Maximum depth of the tree (default: -1, which means no limit)\n",
    "    - subsample: Subsample ratio of the training data (default: 1.0)\n",
    "    - colsample_bytree: Subsample ratio of columns when constructing trees (default: 1.0)\n",
    "    \n",
    "    Returns:\n",
    "    - model: Trained LGBMClassifier model\n",
    "    \"\"\"\n",
    "    warnings.filterwarnings('ignore')\n",
    "    # Initialize the LGBM model with the specified hyperparameters\n",
    "    lgbm_model = LGBMClassifier(\n",
    "        learning_rate=learning_rate,\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        subsample=subsample,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        random_state=0,\n",
    "        verbose = -1\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    lgbm_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = lgbm_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"LGBM Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return lgbm_model\n",
    "\n",
    "\n",
    "def tune_lgbm_optimized(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Perform optimized hyperparameter tuning for LGBMClassifier using RandomizedSearchCV\n",
    "    with reduced search space and early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features (numpy arrays or DataFrames)\n",
    "    - y_train: Training labels (numpy arrays or Series)\n",
    "    \n",
    "    Returns:\n",
    "    - best_model: LGBMClassifier with the best hyperparameters\n",
    "    \"\"\"\n",
    "    # Suppress warnings and output\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    # Initialize the base LGBMClassifier with a reasonable starting configuration\n",
    "    lgbm_model = LGBMClassifier(random_state=42, verbose=-1)\n",
    "    \n",
    "    # Define a reduced hyperparameter grid to search\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.05, 0.1],   # Reduced options\n",
    "        'n_estimators': [50, 100],      # Fewer boosting rounds for faster training\n",
    "        'max_depth': [3, 5],            # Restrict tree depth to prevent overfitting\n",
    "        'subsample': [0.8],             # Fixed subsample ratio\n",
    "        'colsample_bytree': [0.8]       # Fixed column subsample ratio\n",
    "    }\n",
    "    \n",
    "    # Initialize RandomizedSearchCV with reduced iterations and cross-validation folds\n",
    "    randomized_search = RandomizedSearchCV(estimator=lgbm_model,\n",
    "                                           param_distributions=param_grid,\n",
    "                                           n_iter=5,  # Try only 5 random combinations\n",
    "                                           scoring='accuracy',\n",
    "                                           cv=2,  # Use 2-fold cross-validation\n",
    "                                           verbose=0,\n",
    "                                           n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    # Fit RandomizedSearchCV on the training data\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Print the best parameters and cross-validation score\n",
    "    print(f\"Best Hyperparameters: {randomized_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {randomized_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Return the best estimator\n",
    "    return randomized_search.best_estimator_\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def tune_lgbm_optimized(X_train, y_train):\n",
    "    param_grid = {\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8]\n",
    "    }\n",
    "    \n",
    "    # Initialize the base LGBM model\n",
    "    lgbm_model = LGBMClassifier(random_state=42, verbose=-1)\n",
    "    \n",
    "    # Initialize RandomizedSearchCV with 10 iterations and 3-fold CV\n",
    "    randomized_search = RandomizedSearchCV(estimator=lgbm_model,\n",
    "                                           param_distributions=param_grid,\n",
    "                                           n_iter=10,  # Randomly sample 10 combinations\n",
    "                                           scoring='accuracy',\n",
    "                                           cv=3,  # Use 3-fold cross-validation\n",
    "                                           verbose=1,\n",
    "                                           n_jobs=-1)  # Use all available cores\n",
    "    \n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best Hyperparameters: {randomized_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {randomized_search.best_score_:.4f}\")\n",
    "    \n",
    "    return randomized_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8987b37a-d2e6-49a0-b6b6-50552a807a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Accuracy: 0.9556\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.94      0.94      0.94        18\n",
      "           2       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LBGClassifier\n",
    "lgbm_model = train_lgbm(X_train_scaled, X_test_scaled, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "aa99095f-9253-4f3b-8c72-a649ea40d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "best_lgbm_model = tune_lgbm_optimized(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "175aafb7-0fe8-4300-81b1-ee5c2c5f2695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Classifier Accuracy: 0.9556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       0.94      0.94      0.94        18\n",
      "           2       0.91      0.91      0.91        11\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.95      0.95      0.95        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_lgbm_model.predict(X_test_scaled)\n",
    "print(f\"LGBM Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "55756f63-ef3a-4bad-9c9d-69995a651898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Hyperparameters: {'subsample': 0.8, 'n_estimators': 200, 'max_depth': 5, 'learning_rate': 0.05, 'colsample_bytree': 0.8}\n",
      "Best Cross-Validation Accuracy: 0.9429\n"
     ]
    }
   ],
   "source": [
    "# Tuning and Evaluating the lgbm classifier\n",
    "best_lgbm_model = tune_lgbm_optimized(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfa58946-97db-4c68-9d97-81bc3d71dbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBM Classifier Accuracy: 0.9778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best model to make predictions and evaluate\n",
    "y_pred = best_lgbm_model.predict(X_test_scaled)\n",
    "print(f\"LGBM Classifier Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06a62a-fc4f-4daa-9fb1-9c72ae53c1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLENV Py3.12",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
